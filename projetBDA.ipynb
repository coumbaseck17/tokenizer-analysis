{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afe1ed2-68f6-4fc6-8742-2f93296c9f85",
   "metadata": {},
   "source": [
    "# Analyse Comparative des Tokenizers sur des Données Textuelles Multilingues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ca3205-02f8-48be-8c2c-bdf1032e5a82",
   "metadata": {},
   "source": [
    "## Import des librairies nécessaires au chargement et analyse des données\n",
    "\n",
    "- os: donne accès utiles pour le chargement des données du dataset\n",
    "- json: chargement des phrases tokenisées stockées au format json\n",
    "- pandas: chargement des statistiques sur le dataset ainsi que statistiques issues de wikipedia\n",
    "- numpy: manipulation de données vectorielles\n",
    "- scipy: librairie de fonction scientifiques\n",
    "- iso639: permet la manipulation des codes iso-639 identifiant des langues\n",
    "- transformers: permet l'utilisation des tokenizers mise à disposition sur [Hugging Face](https://huggingface.co/)\n",
    "- matplotlib: permet la visualisation de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from the requirements file\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f8006-996b-4a5e-a517-7a67d38fe4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import iso639\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262ef72-0483-4955-9612-e1c9eb8ea76d",
   "metadata": {},
   "source": [
    "## Définition des chemins vers les données à charger\n",
    "\n",
    "- LANG_INFO_PATH: chemin vers le tableau contenant les informations sur les langues dont les phrases ont étés pré-tokenisées\n",
    "- FLORES_DATASET_PATH: chemin vers le dossier content le jeu de données FLORES-200\n",
    "- MADLAD_STATS_PATH: chemin vers le tableau contenant les statistiques sur les données utilisées à l'entraînement du modèle MADLAD-400\n",
    "- WIKI_STATS_PATH: chemin vers le tableau content les statistiques sur wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f976484-9ff1-4ee6-b7eb-58ab674557e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_INFO_PATH = './inputs/FLORES-200.lang_info.csv'\n",
    "FLORES_DATASET_PATH = './inputs/flores200_dataset/'\n",
    "MADLAD_STATS_PATH = './inputs/madlad_stats.tsv'\n",
    "WIKI_STATS_PATH = './inputs/wikipedia_stats.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3e059-ad15-4157-bcbb-434f09c5e3ec",
   "metadata": {},
   "source": [
    "## Selection de la partition de FLORES-200\n",
    "\n",
    "Le jeu de données FLORES-200 est séparé en deux partition:\n",
    "\n",
    "- dev\n",
    "- devtest\n",
    "\n",
    "La variable `SPLIT` permettent de selectionner la partition à charger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f9dea-5e4f-4e88-81ad-4f75891f26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 'dev'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb18b5-9b39-4c30-9294-c73de7ee8540",
   "metadata": {},
   "source": [
    "## Selection des données pré-tokenisées à charger\n",
    "\n",
    "Le jeu de données FLORES-200 a été tokenisé à l'aide des tokenizer des modèles suivant:\n",
    "\n",
    "- [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)\n",
    "- [MADLAD-400](https://huggingface.co/docs/transformers/model_doc/madlad-400)\n",
    "\n",
    "La selection du modèle considéré se fait à travers la variable `MODEL`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c91612-023a-4dc9-8a89-e50da61036af",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'madlad' # 'nllb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e52668-f6d3-4ae4-8644-d8bcf5d4f8b6",
   "metadata": {},
   "source": [
    "Chemin vers le fichier json contenant les données pré-tokenisées:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9b914-9a57-45ed-ae4b-d680ece337ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TOKENIZED_PATH = './inputs/FLORES-200.{:}.tokenized.json'.format(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d895a8-9348-4934-8fb9-4e02dd6eaabc",
   "metadata": {},
   "source": [
    "## Mise en correspondance du modèle sélectionné avec son identifiant unique\n",
    "\n",
    "Le code suivant met en correspondance le nom court du modèle utilisé (`MODEL`) avec son identifiant (`MODEL_NAME`) sur Hugging Face.\n",
    "Cet identifiant sera utilisé plus tard pour charger le tokenizer du modèle sélectionné, ce qui permettra de:\n",
    "\n",
    "- décoder les phrases pré-tokenisées\n",
    "- encoder des phrases pour des nouvelles phrases\n",
    "- avoir accès aux identifiants de token spéciaux, utiles lors de l'analyse des phrases tokenisées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6c62f-82a3-416d-979a-deee3ecedd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'madlad':\n",
    "    MODEL_NAME = 'google/madlad400-3b-mt'\n",
    "    \n",
    "elif MODEL == 'nllb':\n",
    "    MODEL_NAME = 'facebook/nllb-200-3.3B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846eaf07-a085-4446-83fd-bd97d5a802cf",
   "metadata": {},
   "source": [
    "## Chargement des informations sur les données pré-calculées\n",
    "\n",
    "Le tableau csv pointé par `LANG_INFO_PATH` liste les langues pour lesquelles les phrases ont déjà été tokenisées associées avec différentes informations utiles, notamment:\n",
    "\n",
    "- lang_script: le code iso-639-3 ainsi que le script utilisé, cette combinaison correspondant au nom de fichier dans le jeu de données FLORES-200\n",
    "- lang_family: la famille à laquelle apartient la langue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30f887-0206-4358-b698-388989d368b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_info_list = pd.read_csv(LANG_INFO_PATH, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648ce64-08a3-48e7-a697-79bc6122fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282df9c-15fd-4355-9fae-1ef1bc46be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_info_list = pd.read_csv(LANG_INFO_PATH, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279eda1-cc8b-4cd0-8c42-8e5f4867eacd",
   "metadata": {},
   "source": [
    "## Chargement des phrases du dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc0f55-cce7-44f4-b45b-a3f303a3c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du dictionnaire contenant les phrases du dataset pour chaque langue\n",
    "dataset = dict()\n",
    "\n",
    "# Construction du chemin vers le jeu de données à partir de la partition choisie\n",
    "split_base_path = os.path.join(FLORES_DATASET_PATH, SPLIT)\n",
    "\n",
    "\n",
    "# Pour chaque langue identifié par l'index du tableau lang_info_list\n",
    "for lang_id, lang_info in lang_info_list.iterrows():\n",
    "    \n",
    "    # Construction du chemin vers le fichier listant les phrases pour la langue \"lang_id\"\n",
    "    file_path = os.path.join(split_base_path,  '{:}.{:}'.format(lang_info['lang_script'], SPLIT))\n",
    "\n",
    "    # Chargement du fichier listant les phrases\n",
    "    with open(file_path, 'r') as file:\n",
    "        dataset[lang_id] = file.read().split('\\n')\n",
    "\n",
    "        # Suppression de la dernière ligne vide\n",
    "        del(dataset[lang_id][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df99986-3316-4672-af84-5d0d8dde49e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3529dc-2ad2-4d97-b67d-22f3f02501f9",
   "metadata": {},
   "source": [
    "## Chargement des statistiques sur les wikipedias en différentes langues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d23b1-e7e2-47d0-b999-96d98cec47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_stat_list = pd.read_csv(WIKI_STATS_PATH, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d5903-dd0f-4dc8-ac19-1b60952744c1",
   "metadata": {},
   "source": [
    "### Nettoyage des statistiques wikipedia\n",
    "\n",
    "Les lignes ne correspondant pas à un wikipedia d'une langue données sont supprimées et le code iso-639-3 est ajouté\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080f3a3-c189-4d23-ab2c-cd362e2e3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout de la colonne lang_id correspondant au code iso-639-3 pour les langues du tableau\n",
    "wiki_stat_list['lang_id'] = ''\n",
    "\n",
    "# Parcours des identifiants des lignes du tableau\n",
    "for site_id in wiki_stat_list.index:\n",
    "    # Suppression de la ligne si elle ne correspond pas à un sous-domaine de wikipedia\n",
    "    if not site_id.endswith('.wikipedia'):\n",
    "        wiki_stat_list.drop(site_id, inplace=True)\n",
    "        continue\n",
    "\n",
    "    # Extraction du préfix du sous-domaine de wikipedia\n",
    "    site_prefix = site_id.split('.')[0]\n",
    "    \n",
    "    lang_id = None\n",
    "\n",
    "    # La capture d'exception est nécessaire ici dans le cas ou le prefix n'est pas décodable comme code iso-639\n",
    "    try:\n",
    "        # Identification du type de code iso correspondant à la langue\n",
    "        if len(site_prefix) == 2:\n",
    "            # iso-639 part 1\n",
    "            lang_id = iso639.Language.from_part1(site_prefix).part3\n",
    "            \n",
    "        elif len(site_prefix) == 3:\n",
    "            # iso-639 part 3\n",
    "            lang_id = iso639.Language.from_part3(site_prefix).part3\n",
    "\n",
    "        else:\n",
    "            # Trop long pour être un code iso-639\n",
    "            wiki_stat_list.drop(site_id, inplace=True)\n",
    "            continue\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Le préfix n'a pas pu être décodé comme code iso-639\n",
    "        wiki_stat_list.drop(site_id, inplace=True)\n",
    "        continue\n",
    "\n",
    "    # Ajout de l'identifiant iso-639 pour le sous-domaine\n",
    "    wiki_stat_list.loc[site_id, 'lang_id'] = lang_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dec0c7-791e-4bad-a7eb-705056972cb6",
   "metadata": {},
   "source": [
    "### Utilisation du code iso-639 comme index du tableau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15916ea-eff7-47ff-8e6b-d50348f69042",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_stat_list = wiki_stat_list.reset_index().set_index('lang_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6942c0-015a-4edd-b9c3-aa8fd7506f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_stat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37128dd3-8c56-4699-bf88-2f9af323c20d",
   "metadata": {},
   "source": [
    "## Chargement des statistiques sur les données utilisées à l'entraînement du modèle MADLAD-400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d3fcb-f12d-48d6-a692-ea36ae54482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "madlad_stat_list = pd.read_csv(MADLAD_STATS_PATH, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88214035-41af-463a-abe2-1c32c326c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "madlad_stat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a329e3-2128-4e09-bd54-63d43ad13783",
   "metadata": {},
   "source": [
    "## Chargement des données pré-tokenisées\n",
    "\n",
    "### Structure des données\n",
    "\n",
    "Les données tokenisées son représentées dans un dictionnaire au format json structuré de la manière suivante:\n",
    "\n",
    "- dictionnaire des phrases pour chaque langue: `dict<lang_id, list>`\n",
    "  - liste des phrases tokenisées pour une langue donnée: `list<dict>`\n",
    "    - données brutes issues du tokenizer pour une phrase donnée: `dict<string, list>`\n",
    "      - liste des tokens identifiée par la clé `\"input_ids\"`: `list<list<int>>`\n",
    "      - masque d'attention identifié par la clé `\"attention_mask\"`: `list<list<int>>`\n",
    "\n",
    "### Accès à une phrase tokenisée\n",
    "\n",
    "L'accès aux tokens générés pour la première phrase en Anglais se fera par exemple de la manière suivante:\n",
    "\n",
    "```\n",
    "dataset_tokenized['eng'][0]['input_ids'][0]\n",
    "```\n",
    "\n",
    "### Attention\n",
    "\n",
    "- Les données issues du tokenizer ont été stockées telle quelle en suivant un format de batch, elles sont donc représentées par un tableau imbriqué.\n",
    "- Des tokens spéciaux ont automatiquement été insérés, il faudra donc les retirer lors de calcul de charactéristiques des phrases tokenisées. Ces tokens sont par exemple:\n",
    "  - `tokenizer.eos_token_id`: identifiant de début de phrase\n",
    "  - `tokenizer.eos_token_id`: identifiant de fin de phrase\n",
    "  - `tokenizer.unk_token_id`: identifiant de token inconnu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63983e-256a-476d-8016-d20ab80705d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_TOKENIZED_PATH, 'r') as fd:\n",
    "    dataset_tokenized = json.load(fd)[SPLIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ce492-a755-40d6-9aff-2b53412af868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_tokenized['eng'][0]['input_ids'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdbd52-1e8c-4e93-82a5-bd5057f60845",
   "metadata": {},
   "source": [
    "## Calcul d'une charactéristique pour une phrase donnée\n",
    "\n",
    "Cette section montre un exemple de calcul de charactéristique à partir d'une phrase sous forme de chaine de charactères (string) et de sa version tokenisée (list\\<int\\>).\n",
    "\n",
    "La charactéristique calculée ici est le nombre de token moyen produit par mot, son calcul est défini dans une fonction afin de pouvoir être réutilisé plus tard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ee7ac-e12c-4c24-b792-1f11a2b3d5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_per_word_ratio(sentence, sentence_token_list):\n",
    "    # Comptage du nombre de mots\n",
    "    sentence_word_count = len(sentence.split())\n",
    "    \n",
    "    # Comptage du nombre de tokens\n",
    "    sentence_token_count = len(sentence_token_list)\n",
    "    \n",
    "    # Calcul du nombre de tokens produits par mots\n",
    "    sentence_token_per_word_ratio = sentence_token_count / sentence_word_count\n",
    "\n",
    "    return sentence_token_per_word_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e877db-5f38-4724-8ea7-fdfae491ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix de la langue pour la phrase\n",
    "lang_id = 'eng'\n",
    "\n",
    "# Choix du numéro de phrase\n",
    "sentence_index = 0\n",
    "\n",
    "# Récuperation de la phrase selectionnée\n",
    "sentence = dataset[lang_id][sentence_index]\n",
    "\n",
    "# Récupération de la liste des tokens générée pour la phrase\n",
    "sentence_token_list = dataset_tokenized[lang_id][sentence_index]['input_ids'][0]\n",
    "\n",
    "# Calcul de la charactéristique\n",
    "sentence_token_per_word_ratio = compute_token_per_word_ratio(sentence, sentence_token_list)\n",
    "\n",
    "# Affichage de la charactéristique\n",
    "sentence_token_per_word_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1248d95-b028-42ab-aae8-3802d5631c35",
   "metadata": {},
   "source": [
    "## Utilisation du tokenizer\n",
    "\n",
    "Cette section montre comment un tokenizer peut être chargé depuis huggingface et utilisé pour décoder et encoder des phrase.\n",
    "\n",
    "La [documentation des tokenizer huggingface](https://huggingface.co/docs/transformers/main_classes/tokenizer) peut être utile ici.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f1dac2-7231-47f5-a615-d40a5591c2e3",
   "metadata": {},
   "source": [
    "### Chargement du tokenizer pour le modèle considéré\n",
    "\n",
    "la variable MODEL_NAME correspond ici à l'identifiant Hugging Face du modèle pour lequel le tokenizer est chargé, la variable MODEL_NAME peut être remplacée par n'importe quel identifiant de modèle disponible sur Hugging Face.\n",
    "\n",
    "Par exemple pour charger le tokenizer du modèle Phi-3 de Microsoft:\n",
    "\n",
    "```\n",
    "tokenizer = AutoTokenizer.from_pretrained(microsoft/Phi-3-mini-128k-instruct)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccc0003-cd73-4e33-95f2-f0d55ee5ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27ab89-e939-41ce-9778-286d513f37f6",
   "metadata": {},
   "source": [
    "### Exemple de décodage d'une phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206dd70-43af-4a39-8809-5b445c1e9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix de la langue pour la phrase\n",
    "lang_id = 'eng'\n",
    "\n",
    "# Choix du numéro de phrase\n",
    "sentence_index = 0\n",
    "\n",
    "# Récuperation de la liste des tokens pour la phrase selectionnée\n",
    "sentence_token_list = dataset_tokenized[lang_id][sentence_index]['input_ids'][0]\n",
    "\n",
    "# Décodage à l'aide du tokenizer\n",
    "sentence_decoded = tokenizer.decode(sentence_token_list, skip_special_tokens=True)\n",
    "\n",
    "# Affichage de la phrase décodée\n",
    "sentence_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05bcff-00ff-4275-b554-17887b75a1a5",
   "metadata": {},
   "source": [
    "### Exemple d'encodage d'une phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef20239-c4a3-41b5-a639-a3ff5c79c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix de la langue pour la phrase\n",
    "lang_id = 'eng'\n",
    "\n",
    "# Choix du numéro de phrase\n",
    "sentence_index = 0\n",
    "\n",
    "# Récuperation de la phrase selectionnée\n",
    "sentence = dataset[lang_id][sentence_index]\n",
    "\n",
    "# Décodage à l'aide du tokenizer\n",
    "# L'argument add_special_tokens=False permet d'éviter de générer les tokens spéciaux tel que les tokens de début et fin de phrase\n",
    "sentence_encoded = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "\n",
    "# Affichage de la phrase décodée\n",
    "sentence_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d868d9-a575-45a3-a3bd-6dfb7997e188",
   "metadata": {},
   "source": [
    "### Décodage token par token d'une phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf649fb0-53fc-4935-9155-8c5f0e8e552b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choix de la langue pour la phrase\n",
    "lang_id = 'eng'\n",
    "\n",
    "# Choix du numéro de phrase\n",
    "sentence_index = 0\n",
    "\n",
    "for token_id in dataset_tokenized[lang_id][sentence_index]['input_ids'][0]:\n",
    "    decoded_token = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "    print('{:}:\\t\"{:}\"'.format(token_id, decoded_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a2ff32-2047-459b-9e91-9d9e18a65c97",
   "metadata": {},
   "source": [
    "### Encodage mot par mot d'une phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765ad1f-f126-4830-8e12-35855b8cf9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix de la langue pour la phrase\n",
    "lang_id = 'eng'\n",
    "\n",
    "# Choix du numéro de phrase\n",
    "sentence_index = 0\n",
    "\n",
    "# Récupération de la phrase\n",
    "sentence = dataset[lang_id][sentence_index]\n",
    "\n",
    "# Séparation de la phrase en mots\n",
    "sentence_word_list = sentence.split()\n",
    "\n",
    "for word in sentence_word_list:\n",
    "    word_token_list = tokenizer.encode(word, add_special_tokens=False)\n",
    "    print('\"{:}\":\\n\\t{:}'.format(word, word_token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a21e87-d973-42ec-b1be-c7fcadf173d5",
   "metadata": {},
   "source": [
    "### Encodage du jeu de données entier à l'aide du tokenizer nouvellement chargé\n",
    "\n",
    "Cette section montre comment l'ensemble du jeu de données peut être encodé à l'aide du tokenizer nouvellement chargé.\n",
    "\n",
    "#### Attention\n",
    "\n",
    "Les données produites ici ne seront pas identiques à celles présentes dans `dataset_tokenized` pour les raisons suivantes:\n",
    "\n",
    "- L'encodage se fait ici phrase par phrase et non par batch, produisant un liste simple de token (list\\<int\\>) et non une liste imbriquée (list\\<list\\<int\\>\\>)\n",
    "- l'encodage phrase par phrase ne génére qu'une liste de token et non un dictionnaire contenant la liste de token (`input_ids`) et le masque (`attention_mask`).\n",
    "- La génération de token spéciaux tel que `tokenizer.eos_token_id` et `tokenizer.bos_token_id` est désactivée\n",
    "\n",
    "La structure produit aura le format suivant:\n",
    "\n",
    "- dictionnaire des phrases pour chaque langue: `dict<lang_id, list>`\n",
    "  - liste des phrases tokenisées pour une langue donnée: `list<list>`\n",
    "    - liste des tokens: `list<int>`\n",
    "\n",
    "#### Accès à une phrase tokenisée\n",
    "\n",
    "L'accès aux tokens générés pour la première phrase en Anglais se fera par exemple de la manière suivante:\n",
    "\n",
    "```\n",
    "dataset_tokenized_new['eng'][0]\n",
    "```\n",
    "\n",
    "La ou pour les données pré-calculées l'accès se faisait de la manière suivante:\n",
    "\n",
    "```\n",
    "dataset_tokenized_new['eng'][0]['input_ids'][0]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de61767-0603-4b55-9548-aa42ee854dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokenized_new = dict()\n",
    "\n",
    "# Pour chaque langue identifié par l'index du tableau lang_info_list\n",
    "for lang_id in lang_info_list.index:\n",
    "    # Insertion de la liste contenant les phrases encodées pour la langue\n",
    "    dataset_tokenized_new[lang_id] = list()\n",
    "\n",
    "    # Parcours des phrases pour la langue donnée\n",
    "    for sentence in dataset[lang_id]:\n",
    "        # Encodage de la phrase\n",
    "        sentence_token_list = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "\n",
    "        # Enregistrement de la liste des tokens\n",
    "        dataset_tokenized_new[lang_id].append(sentence_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcd9a8a-646b-4bd2-9b8e-bb2facfbeaf1",
   "metadata": {},
   "source": [
    "### Comparaison avec les données pré-calculés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d269237-845d-4823-8ee1-8418367cf4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Données précalculées:')\n",
    "print(dataset_tokenized['eng'][0]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2922b6-ad7f-4e6c-826d-070e8050fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Données fraîchement calculées:')\n",
    "print(dataset_tokenized_new['eng'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6c9ee-ea57-4acb-aa30-0f09ff53f2c7",
   "metadata": {},
   "source": [
    "## Mise en pratique\n",
    "\n",
    "Dans cette section vous devrez utiliser les connaissances acquises dans les sections précédentes et:\n",
    "\n",
    "- calculer des charactéristiques des phrases du jeu de données\n",
    "- analyser ces charactéristiques\n",
    "- visualiser ces charactéristiques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad92e23-a2a4-4f7b-a751-dda5f91eef4f",
   "metadata": {},
   "source": [
    "### Calcul de charactéristiques\n",
    "\n",
    "Remplir le dictionnaire `dataset_features` avec les valeurs d'une charactéristique dont vous aurez défini le calcul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86fe9b-dd69-4561-b900-db54fefa9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_1(sentence, token_list):\n",
    "    return len(sentence) * (1 + np.random.rand())\n",
    "\n",
    "\n",
    "def compute_feature_2(sentence, token_list):\n",
    "    return len(sentence) * (-1 + np.random.rand())\n",
    "\n",
    "\n",
    "dataset_feature_list = dict()\n",
    "\n",
    "dataset_feature_list['feature_1'] = dict()\n",
    "dataset_feature_list['feature_2'] = dict()\n",
    "\n",
    "for lang_id in lang_info_list.index:\n",
    "    dataset_feature_list['feature_1'][lang_id] = list()\n",
    "    dataset_feature_list['feature_2'][lang_id] = list()\n",
    "\n",
    "    for sentence, token_list in zip(dataset[lang_id], dataset_tokenized_new[lang_id]):\n",
    "        sentence_feature_1 = compute_feature_1(sentence, token_list)\n",
    "        sentence_feature_2 = compute_feature_2(sentence, token_list)\n",
    "        \n",
    "        dataset_feature_list['feature_1'][lang_id].append(sentence_feature_1)\n",
    "        dataset_feature_list['feature_2'][lang_id].append(sentence_feature_2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b17920-71eb-4910-90b6-328eddca06b9",
   "metadata": {},
   "source": [
    "### Analyse de charactéristiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa13438-2e14-4b2d-9faf-9c1e213f3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise à plat des données\n",
    "dataset_feature_list_flat = dict()\n",
    "\n",
    "\n",
    "for feature in dataset_feature_list:\n",
    "    dataset_feature_list_flat[feature] = list()\n",
    "    for lang_id in sorted(lang_info_list.index):\n",
    "        dataset_feature_list_flat[feature].extend(dataset_feature_list[feature][lang_id])\n",
    "\n",
    "feature_1_2_pearson = scipy.stats.pearsonr(dataset_feature_list_flat['feature_1'], dataset_feature_list_flat['feature_2'])\n",
    "\n",
    "feature_1_2_pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399de970-bf62-401e-a34f-e9a9bd36b618",
   "metadata": {},
   "source": [
    "### Visualisation des charactéristiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a196ec6-55e4-4f79-9c45-919c5a09325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(dataset_feature_list_flat['feature_1'], dataset_feature_list_flat['feature_2'], s=1)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
